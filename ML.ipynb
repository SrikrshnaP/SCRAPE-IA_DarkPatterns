{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d12d0aa",
   "metadata": {},
   "source": [
    "# FASTER RCNN (Prebuilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_classes = [\"LIKE\", \"DISLIKE\", \"STAR\", \"TSON\", \"AD\", \"ADLOADER\"]\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance(num_classes):\n",
    "    # load a model instance pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # print(model)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1695fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 6\n",
    "\n",
    "# model = get_model_instance(num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6be3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.load_state_dict(torch.load(\"C:\\\\Users\\\\91998\\\\runs\\\\detect\\\\train15\\\\weights\\\\best.pt\"))\n",
    "# checkpoint = torch.load('C:\\\\Users\\\\91998\\\\Downloads\\\\trained-visual-cue-detection-model.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "checkpoint = torch.load(\"C:\\\\Users\\\\91998\\\\runs\\\\detect\\\\train15\\\\weights\\\\best.pt\", map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222fdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d551f1",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43af7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pyautogui\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc596154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model,i):\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    screenshot = screenshot.resize((640, 640))\n",
    "    image = screenshot.convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    input_data = transform(image).unsqueeze(0) \n",
    "    output = model(input_data)\n",
    "    print(output)\n",
    "\n",
    "    # Get only >75%\n",
    "    bounding_boxes = []\n",
    "    for i, score in enumerate(output[0]['scores']):\n",
    "        if score >= 0.75:\n",
    "            bounding_boxes.append(i)\n",
    "\n",
    "    # Make bounding boxes on all of them\n",
    "    image = cv2.cvtColor(numpy.array(image), cv2.COLOR_RGB2BGR)\n",
    "    for box in bounding_boxes:\n",
    "        print(box)\n",
    "        # Define the coordinates of the top-left corner\n",
    "        # Define the width and height of the bounding box\n",
    "        x, y, x2, y2 = map(int, output[0]['boxes'][box])\n",
    "        print(2)\n",
    "        # Calculate the coordinates of the other corners of the bounding box\n",
    "        # x2, y2 = x + width, y + height\n",
    "\n",
    "        # Draw the bounding box on the image\n",
    "        color = (0, 255, 0)  # Green color, you can change it as needed\n",
    "        thickness = 2  # Line thickness\n",
    "        cv2.rectangle(image, (x, y), (x2, y2), color, thickness)\n",
    "    print(3)\n",
    "    # Display the image with the bounding box\n",
    "    cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('output', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.imwrite(f'C:\\\\Users\\\\91998\\\\Downloads\\\\DarkPatterns\\\\downloaded_images\\\\test{i}.png', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c362d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(5):\n",
    "    pipeline(model,i)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d1475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1080 x 1920\n",
    "screenshot = pyautogui.screenshot()\n",
    "screenshot = screenshot.resize((1080, 1920))\n",
    "screenshot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ed2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_path = \"C:\\\\Users\\\\91998\\\\Downloads\\\\AidUI-Object-Detection-Dataset-Master\\\\AidUI-Object-Detection-Dataset-Master\\\\10726_DISLIKE_2.png\"\n",
    "# image_path = \"C:\\\\Users\\\\91998\\\\Downloads\\\\amazon.png\"\n",
    "# image_path = \"C:\\\\Users\\\\91998\\\\Downloads\\\\AidUI-Evaluation-Dataset\\\\evaluation_dataset\\\\web\\\\images\\\\centralvapors.com.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),  # Adjust size if needed\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "# transforms.Resize(224),\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                      #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      #                     [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "\n",
    "input_data = transform(image).unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5adbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image=image.resize((64,64))\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317879d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e81b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9c524",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed58eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0]['scores'][0] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff481e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0]['boxes'][0]\n",
    "output[0]['scores']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862922d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "print(1)\n",
    "# Define the coordinates of the top-left corner\n",
    "# Define the width and height of the bounding box\n",
    "x, y, x2, y2 = map(int, output[0]['boxes'][0])\n",
    "print(2)\n",
    "# Calculate the coordinates of the other corners of the bounding box\n",
    "# x2, y2 = x + width, y + height\n",
    "\n",
    "# Draw the bounding box on the image\n",
    "color = (0, 255, 0)  # Green color, you can change it as needed\n",
    "thickness = 2  # Line thickness\n",
    "cv2.rectangle(image, (x, y), (x2, y2), color, thickness)\n",
    "print(3)\n",
    "# Display the image with the bounding box\n",
    "cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('output', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90734d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('output', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59644d",
   "metadata": {},
   "source": [
    "# YOLO V8\n",
    "\n",
    "https://stackoverflow.com/questions/4081064/how-to-get-the-number-of-pixels-a-user-has-scrolled-down-the-page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.todevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model with 2 GPUs\n",
    "results = model.train(data='C:/Users/91998/Downloads/DarkPatterns/dataset.yml', epochs=10, imgsz=1280, device=[0], patience=5)\n",
    "# \"C:\\Users\\91998\\Downloads\\DarkPatterns\\output\\dataset.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84515018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO('yolov8n.yaml')  # build a new model from YAML\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "# model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data='C:/Users/91998/Downloads/DarkPatterns/dataset.yml', epochs=10, imgsz=1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47a72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "# model = YOLO('C:\\\\Users\\\\91998\\\\Downloads\\\\DarkPatterns\\\\yolov8n.pt')\n",
    "model = YOLO(\"C:\\\\Users\\\\91998\\\\runs\\\\detect\\\\train15\\\\weights\\\\best.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e54117f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 C:\\Users\\91998\\Downloads\\DarkPatterns\\output\\images\\train\\images\\(688)www.chicwish.com_4cb6_BOMB_3.png: 704x1280 1 BOMB, 271.2ms\n",
      "Speed: 14.9ms preprocess, 271.2ms inference, 1.0ms postprocess per image at shape (1, 3, 704, 1280)\n"
     ]
    }
   ],
   "source": [
    "# Run inference on 'bus.jpg'\n",
    "# results = model(\"C:\\\\Users\\\\91998\\\\Downloads\\\\checkbox.png\")  # results list\n",
    "im_path = \"C:\\\\Users\\\\91998\\\\Downloads\\\\DarkPatterns\\\\output\\\\images\\\\train\\\\images\\\\(688)www.chicwish.com_4cb6_BOMB_3.png\"\n",
    "image = cv2.imread(im_path)\n",
    "results = model(im_path)\n",
    "# Show the results\n",
    "for r in results:\n",
    "    for i, prob in enumerate(r.boxes.conf):\n",
    "        if prob > 0.5:\n",
    "            # im_array = r.plot()  # plot a BGR numpy array of predictions\n",
    "            # im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "            # im.show()  # show image\n",
    "            # im.save('results.jpg')  # save image\n",
    "\n",
    "            color = (0, 255, 0)  # Green color, you can change it as needed\n",
    "            thickness = 2  # Line thickness\n",
    "            x, y, x2, y2 = map(int, r.boxes.xywh[i])\n",
    "            cv2.rectangle(image, (x, y), (x2, y2), color, thickness)\n",
    "            # print(3)\n",
    "            # Display the image with the bounding box\n",
    "    cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('output', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a7dfc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([2.])\n",
       "conf: tensor([0.9534])\n",
       "data: tensor([[1.6752e+03, 8.7411e+02, 1.7023e+03, 9.1864e+02, 9.5344e-01, 2.0000e+00]])\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (1300, 2400)\n",
       "shape: torch.Size([1, 6])\n",
       "xywh: tensor([[1688.7686,  896.3747,   27.0760,   44.5347]])\n",
       "xywhn: tensor([[0.7037, 0.6895, 0.0113, 0.0343]])\n",
       "xyxy: tensor([[1675.2305,  874.1074, 1702.3065,  918.6420]])\n",
       "xyxyn: tensor([[0.6980, 0.6724, 0.7093, 0.7066]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "results[0].boxes\n",
    "# results[0].probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8494f3e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21436\\642557003.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming Tensor conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming Tensor conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    boxes = r.boxes.numpy()  # Assuming Tensor conversion\n",
    "    probs = r.probs.numpy()  # Assuming Tensor conversion\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box  # Extract box coordinates (replace with correct indices)\n",
    "        class_id = np.argmax(probs)  # Assuming first index is confidence, others are class probs\n",
    "        class_prob = probs[class_id]  # Access probability for detected class\n",
    "\n",
    "        print(f\"Bounding box: ({x1}, {y1}) - ({x2}, {y2}), Class {class_id} with probability {class_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f24fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pyautogui\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0254b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model,i):\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    image = screenshot.convert(\"RGB\")\n",
    "    \n",
    "    results = model(image)\n",
    "    # print(output)\n",
    "\n",
    "    for r in results:\n",
    "        im_array = r.plot()  # plot a BGR numpy array of predictions\n",
    "        im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "        im.show()  # show image\n",
    "        im.save(f'C:\\\\Users\\\\91998\\\\Downloads\\\\DarkPatterns\\\\downloaded_images\\\\test{i}.png')\n",
    "    print(3)\n",
    "    # Display the image with the bounding box\n",
    "    # cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL)\n",
    "    # cv2.imshow('output', image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.imwrite(f'C:\\\\Users\\\\91998\\\\Downloads\\\\DarkPatterns\\\\downloaded_images\\\\test{i}.png', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e81b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 800x1280 (no detections), 382.2ms\n",
      "Speed: 14.3ms preprocess, 382.2ms inference, 1.1ms postprocess per image at shape (1, 3, 800, 1280)\n",
      "3\n",
      "\n",
      "0: 800x1280 1 CHECKBOX, 242.2ms\n",
      "Speed: 9.1ms preprocess, 242.2ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 1280)\n",
      "3\n",
      "\n",
      "0: 800x1280 1 BOMB, 244.8ms\n",
      "Speed: 11.3ms preprocess, 244.8ms inference, 1.2ms postprocess per image at shape (1, 3, 800, 1280)\n",
      "3\n",
      "\n",
      "0: 800x1280 (no detections), 260.0ms\n",
      "Speed: 12.7ms preprocess, 260.0ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 1280)\n",
      "3\n",
      "\n",
      "0: 800x1280 (no detections), 262.1ms\n",
      "Speed: 10.3ms preprocess, 262.1ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 1280)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(5):\n",
    "    pipeline(model,i)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5275e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55312cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
